{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.OpenAI(model='gpt-3.5-turbo',\n",
    "                  api_base='https://free-chatgpt-fqhs.onrender.com/v1',\n",
    "                  api_key='123456')\n",
    "\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Question: 你是谁？\n",
      "Answer: ChatGPT\n"
     ]
    }
   ],
   "source": [
    "# Define the predictor.\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "# Call the predictor on a particular input.\n",
    "pred = generate_answer(question=\"你是谁？\")\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReader(dspy.Signature):\n",
    "    \"\"\"你是一个代码专家，对代码的每一个类，每个方法,每个函数的用途参数含义，参数类型，参数含义和类型进行说明.\n",
    "回答以markdown格式，回答要有条理，有分行分段。用中文进行回答\"\"\"\n",
    "\n",
    "    filename = dspy.InputField(desc=\"文件名称\")\n",
    "    code = dspy.InputField(desc=\"代码内容\")\n",
    "    explain = dspy.OutputField(desc=\"对代码的解读\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reader = dspy.Predict(CodeReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modeling_chatglm.py'\n",
    "code = '''\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, LayerNorm\n",
    "from torch.nn import CrossEntropyLoss, LayerNorm, MSELoss, BCEWithLogitsLoss\n",
    "from torch.nn.utils import skip_init\n",
    "from typing import Optional, Tuple, Union, List, Callable, Dict, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "from transformers.generation.logits_process import LogitsProcessor\n",
    "from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig, ModelOutput\n",
    "\n",
    "from .configuration_chatglm import ChatGLMConfig\n",
    "\n",
    "# flags required to enable jit fusion kernels\n",
    "\n",
    "if sys.platform != 'darwin':\n",
    "    torch._C._jit_set_profiling_mode(False)\n",
    "    torch._C._jit_set_profiling_executor(False)\n",
    "    torch._C._jit_override_can_fuse_on_cpu(True)\n",
    "    torch._C._jit_override_can_fuse_on_gpu(True)\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"THUDM/ChatGLM2-6B\"\n",
    "_CONFIG_FOR_DOC = \"ChatGLM6BConfig\"\n",
    "\n",
    "CHATGLM_6B_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"THUDM/chatglm2-6b\",\n",
    "    # See all ChatGLM models at https://huggingface.co/models?filter=chatglm\n",
    "]\n",
    "\n",
    "\n",
    "def default_init(cls, *args, **kwargs):\n",
    "    return cls(*args, **kwargs)\n",
    "\n",
    "\n",
    "class InvalidScoreLogitsProcessor(LogitsProcessor):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        if torch.isnan(scores).any() or torch.isinf(scores).any():\n",
    "            scores.zero_()\n",
    "            scores[..., 5] = 5e4\n",
    "        return scores\n",
    "\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The torch.nn model to encode the prefix\n",
    "    Input shape: (batch-size, prefix-length)\n",
    "    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ChatGLMConfig):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        if self.prefix_projection:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            kv_size = config.num_layers * config.kv_channels * config.multi_query_group_num * 2\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len, kv_size)\n",
    "            self.trans = torch.nn.Sequential(\n",
    "                torch.nn.Linear(kv_size, config.hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(config.hidden_size, kv_size)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len,\n",
    "                                                config.num_layers * config.kv_channels * config.multi_query_group_num * 2)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.trans(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values\n",
    "\n",
    "\n",
    "def split_tensor_along_last_dim(\n",
    "        tensor: torch.Tensor,\n",
    "        num_partitions: int,\n",
    "        contiguous_split_chunks: bool = False,\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"Split a tensor along its last dimension.\n",
    "\n",
    "    Arguments:\n",
    "        tensor: input tensor.\n",
    "        num_partitions: number of partitions to split the tensor\n",
    "        contiguous_split_chunks: If True, make each chunk contiguous\n",
    "                                 in memory.\n",
    "\n",
    "    Returns:\n",
    "        A list of Tensors\n",
    "    \"\"\"\n",
    "    # Get the size and dimension.\n",
    "    last_dim = tensor.dim() - 1\n",
    "    last_dim_size = tensor.size()[last_dim] // num_partitions\n",
    "    # Split.\n",
    "    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n",
    "    # Note: torch.split does not create contiguous tensors by default.\n",
    "    if contiguous_split_chunks:\n",
    "        return tuple(chunk.contiguous() for chunk in tensor_list)\n",
    "\n",
    "    return tensor_list\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, rope_ratio=1, original_impl=False, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).to(dtype=dtype) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.dim = dim\n",
    "        self.original_impl = original_impl\n",
    "        self.rope_ratio = rope_ratio\n",
    "\n",
    "    def forward_impl(\n",
    "            self, seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    "    ):\n",
    "        \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "        Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "        transformers/rope/__init__.py. MIT License:\n",
    "        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "        \"\"\"\n",
    "        # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "        base = base * self.rope_ratio\n",
    "        theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=torch.float, device=device) / n_elem))\n",
    "\n",
    "        # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "        seq_idx = torch.arange(seq_len, dtype=torch.float, device=device)\n",
    "\n",
    "        # Calculate the product of position index and $\\theta_i$\n",
    "        idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "        # this is to mimic the behaviour of complex32, else we will get different results\n",
    "        if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "            cache = cache.bfloat16() if dtype == torch.bfloat16 else cache.half()\n",
    "        return cache\n",
    "\n",
    "    def forward(self, max_seq_len, offset=0):\n",
    "        return self.forward_impl(\n",
    "            max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n",
    "    # x: [sq, b, np, hn]\n",
    "    sq, b, np, hn = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "    rot_dim = rope_cache.shape[-2] * 2\n",
    "    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]\n",
    "    # truncate to support variable sizes\n",
    "    rope_cache = rope_cache[:sq]\n",
    "    xshaped = x.reshape(sq, -1, np, rot_dim // 2, 2)\n",
    "    rope_cache = rope_cache.view(sq, -1, 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return torch.cat((x_out2, x_pass), dim=-1)\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, device=None, dtype=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.empty(normalized_shape, device=device, dtype=dtype))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "\n",
    "class CoreAttention(torch.nn.Module):\n",
    "    def __init__(self, config: ChatGLMConfig, layer_number):\n",
    "        super(CoreAttention, self).__init__()\n",
    "\n",
    "        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling\n",
    "        self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32\n",
    "        if self.apply_query_key_layer_scaling:\n",
    "            self.attention_softmax_in_fp32 = True\n",
    "        self.layer_number = max(1, layer_number)\n",
    "\n",
    "        projection_size = config.kv_channels * config.num_attention_heads\n",
    "\n",
    "        # Per attention head and per partition values.\n",
    "        self.hidden_size_per_partition = projection_size\n",
    "        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads\n",
    "        self.num_attention_heads_per_partition = config.num_attention_heads\n",
    "\n",
    "        coeff = None\n",
    "        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)\n",
    "        if self.apply_query_key_layer_scaling:\n",
    "            coeff = self.layer_number\n",
    "            self.norm_factor *= coeff\n",
    "        self.coeff = coeff\n",
    "\n",
    "        self.attention_dropout = torch.nn.Dropout(config.attention_dropout)\n",
    "\n",
    "    def forward(self, query_layer, key_layer, value_layer, attention_mask):\n",
    "        pytorch_major_version = int(torch.__version__.split('.')[0])\n",
    "        if pytorch_major_version >= 2:\n",
    "            query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]\n",
    "            if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:\n",
    "                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n",
    "                                                                                 is_causal=True)\n",
    "            else:\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = ~attention_mask\n",
    "                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n",
    "                                                                                 attention_mask)\n",
    "            context_layer = context_layer.permute(2, 0, 1, 3)\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n",
    "            context_layer = context_layer.reshape(*new_context_layer_shape)\n",
    "        else:\n",
    "            # Raw attention scores\n",
    "\n",
    "            # [b, np, sq, sk]\n",
    "            output_size = (query_layer.size(1), query_layer.size(2), query_layer.size(0), key_layer.size(0))\n",
    "\n",
    "            # [sq, b, np, hn] -> [sq, b * np, hn]\n",
    "            query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)\n",
    "            # [sk, b, np, hn] -> [sk, b * np, hn]\n",
    "            key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)\n",
    "\n",
    "            # preallocting input tensor: [b * np, sq, sk]\n",
    "            matmul_input_buffer = torch.empty(\n",
    "                output_size[0] * output_size[1], output_size[2], output_size[3], dtype=query_layer.dtype,\n",
    "                device=query_layer.device\n",
    "            )\n",
    "\n",
    "            # Raw attention scores. [b * np, sq, sk]\n",
    "            matmul_result = torch.baddbmm(\n",
    "                matmul_input_buffer,\n",
    "                query_layer.transpose(0, 1),  # [b * np, sq, hn]\n",
    "                key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n",
    "                beta=0.0,\n",
    "                alpha=(1.0 / self.norm_factor),\n",
    "            )\n",
    "\n",
    "            # change view to [b, np, sq, sk]\n",
    "            attention_scores = matmul_result.view(*output_size)\n",
    "\n",
    "            # ===========================\n",
    "            # Attention probs and dropout\n",
    "            # ===========================\n",
    "\n",
    "            # attention scores and attention mask [b, np, sq, sk]\n",
    "            if self.attention_softmax_in_fp32:\n",
    "                attention_scores = attention_scores.float()\n",
    "            if self.coeff is not None:\n",
    "                attention_scores = attention_scores * self.coeff\n",
    "            if attention_mask is None and attention_scores.shape[2] == attention_scores.shape[3]:\n",
    "                attention_mask = torch.ones(output_size[0], 1, output_size[2], output_size[3],\n",
    "                                            device=attention_scores.device, dtype=torch.bool)\n",
    "                attention_mask.tril_()\n",
    "                attention_mask = ~attention_mask\n",
    "            if attention_mask is not None:\n",
    "                attention_scores = attention_scores.masked_fill(attention_mask, float(\"-inf\"))\n",
    "            attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "            attention_probs = attention_probs.type_as(value_layer)\n",
    "\n",
    "            # This is actually dropping out entire tokens to attend to, which might\n",
    "            # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "            attention_probs = self.attention_dropout(attention_probs)\n",
    "            # =========================\n",
    "            # Context layer. [sq, b, hp]\n",
    "            # =========================\n",
    "\n",
    "            # value_layer -> context layer.\n",
    "            # [sk, b, np, hn] --> [b, np, sq, hn]\n",
    "\n",
    "            # context layer shape: [b, np, sq, hn]\n",
    "            output_size = (value_layer.size(1), value_layer.size(2), query_layer.size(0), value_layer.size(3))\n",
    "            # change view [sk, b * np, hn]\n",
    "            value_layer = value_layer.view(value_layer.size(0), output_size[0] * output_size[1], -1)\n",
    "            # change view [b * np, sq, sk]\n",
    "            attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "            # matmul: [b * np, sq, hn]\n",
    "            context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))\n",
    "            # change view [b, np, sq, hn]\n",
    "            context_layer = context_layer.view(*output_size)\n",
    "            # [b, np, sq, hn] --> [sq, b, np, hn]\n",
    "            context_layer = context_layer.permute(2, 0, 1, 3).contiguous()\n",
    "            # [sq, b, np, hn] --> [sq, b, hp]\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n",
    "            context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = code_reader(filename=filename, code=code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "# Filename: modeling_chatglm.py\n",
      "\n",
      "## default_init\n",
      "### Parameters:\n",
      "- cls: class\n",
      "- *args: positional arguments\n",
      "- **kwargs: keyword arguments\n",
      "### Explanation:\n",
      "This function serves as a default initializer for classes.\n",
      "\n",
      "## InvalidScoreLogitsProcessor\n",
      "### Parameters:\n",
      "- LogitsProcessor: LogitsProcessor object\n",
      "### Explanation:\n",
      "This class is a subclass of LogitsProcessor and is used to process invalid scores in logits.\n",
      "\n",
      "## PrefixEncoder\n",
      "### Parameters:\n",
      "- config: ChatGLMConfig object\n",
      "### Explanation:\n",
      "This class encodes the prefix using a two-layer MLP or a simple embedding based on the configuration provided.\n",
      "\n",
      "## split_tensor_along_last_dim\n",
      "### Parameters:\n",
      "- tensor: torch.Tensor\n",
      "- num_partitions: int\n",
      "- contiguous_split_chunks: bool (default=False)\n",
      "### Explanation:\n",
      "This function splits a tensor along its last dimension into a specified number of partitions.\n",
      "\n",
      "## RotaryEmbedding\n",
      "### Parameters:\n",
      "- dim: int\n",
      "- rope_ratio: float (default=1)\n",
      "- original_impl: bool (default=False)\n",
      "- device: torch.device (default=None)\n",
      "- dtype: torch.dtype (default=None)\n",
      "### Explanation:\n",
      "This class implements Rotary Position Embedding to enhance Transformer models.\n",
      "\n",
      "## apply_rotary_pos_emb\n",
      "### Parameters:\n",
      "- x: torch.Tensor\n",
      "- rope_cache: torch.Tensor\n",
      "### Explanation:\n",
      "This function applies rotary position embedding to a tensor.\n",
      "\n",
      "## RMSNorm\n",
      "### Parameters:\n",
      "- normalized_shape: int\n",
      "- eps: float (default=1e-5)\n",
      "- device: torch.device (default=None)\n",
      "- dtype: torch.dtype (default=None)\n",
      "### Explanation:\n",
      "This class implements Root Mean Square Layer Normalization.\n",
      "\n",
      "## CoreAttention\n",
      "### Parameters:\n",
      "- config: ChatGLMConfig object\n",
      "- layer_number: int\n",
      "### Explanation:\n",
      "This class implements the core attention mechanism used in the ChatGLM model.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(pred.explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
